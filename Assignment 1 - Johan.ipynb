{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 1:\n",
    "### 1: Calculate the total word count for each novel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing required libraries:\n",
    "import os #For interacting with operating systems.\n",
    "from collections import Counter #Import the conter-function from the collections library for counting unique words.\n",
    "import re #For removing non-characters.\n",
    "from pathlib import Path #Importing the Path-function from the pathlib package.\n",
    "import pandas as pd  #For creating a dataframe which can be exported as a .csv-file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = os.path.join(\"..\",\"data\", \"100_english_novels\", \"corpus\") #Specifying a filepath for loading data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cbronte_Villette_1853.txt\n",
      "Forster_Angels_1905.txt\n",
      "Woolf_Lighthouse_1927.txt\n",
      "Meredith_Richmond_1871.txt\n",
      "Stevenson_Treasure_1883.txt\n",
      "Forster_Howards_1910.txt\n",
      "Wcollins_Basil_1852.txt\n",
      "Schreiner_Undine_1929.txt\n",
      "Galsworthy_Man_1906.txt\n",
      "Corelli_Innocent_1914.txt\n",
      "Kipling_Light_1891.txt\n",
      "Conrad_Nostromo_1904.txt\n",
      "Stevenson_Arrow_1888.txt\n",
      "Hardy_Tess_1891.txt\n",
      "Thackeray_Esmond_1852.txt\n",
      "Doyle_Lost_1912.txt\n",
      "Trollope_Angel_1881.txt\n",
      "Gissing_Warburton_1903.txt\n",
      "Barclay_Rosary_1909.txt\n",
      "Eliot_Daniel_1876.txt\n",
      "James_Tragic_1890.txt\n",
      "Doyle_Micah_1889.txt\n",
      "Dickens_Bleak_1853.txt\n",
      "Woolf_Night_1919.txt\n",
      "Braddon_Audley_1862.txt\n",
      "Bennet_Babylon_1902.txt\n",
      "Kipling_Kim_1901.txt\n",
      "Lytton_What_1858.txt\n",
      "Meredith_Marriage_1895.txt\n",
      "Corelli_Satan_1895.txt\n",
      "Haggard_Mines_1885.txt\n",
      "Stevenson_Catriona_1893.txt\n",
      "Doyle_Hound_1902.txt\n",
      "Chesterton_Innocence_1911.txt\n",
      "Blackmore_Lorna_1869.txt\n",
      "Haggard_Sheallan_1921.txt\n",
      "Gaskell_Wives_1865.txt\n",
      "Cbronte_Jane_1847.txt\n",
      "Wcollins_Legacy_1889.txt\n",
      "Morris_Roots_1890.txt\n",
      "Burnett_Garden_1911.txt\n",
      "Ford_Post_1926.txt\n",
      "Thackeray_Pendennis_1850.txt\n",
      "James_Roderick_1875.txt\n",
      "Haggard_She_1887.txt\n",
      "Galsworthy_River_1933.txt\n",
      "Morris_Wood_1894.txt\n",
      "Barclay_Postern_1911.txt\n",
      "Conrad_Almayer_1895.txt\n",
      "Thackeray_Virginians_1859.txt\n",
      "Bennet_Helen_1910.txt\n",
      "Lee_Brown_1884.txt\n",
      "Lawrence_Women_1920.txt\n",
      "Schreiner_Farm_1883.txt\n",
      "Lytton_Novel_1853.txt\n",
      "Lawrence_Peacock_1911.txt\n",
      "Schreiner_Trooper_1897.txt\n",
      "Cbronte_Shirley_1849.txt\n",
      "James_Ambassadors_1903.txt\n",
      "Lawrence_Serpent_1926.txt\n",
      "Braddon_Quest_1871.txt\n",
      "Dickens_Oliver_1839.txt\n",
      "Trollope_Warden_1855.txt\n",
      "Barclay_Ladies_1917.txt\n",
      "Ward_Harvest_1920.txt\n",
      "Blackmore_Erema_1877.txt\n",
      "Wcollins_Woman_1860.txt\n",
      "Hardy_Madding_1874.txt\n",
      "Lee_Penelope_1903.txt\n",
      "Eliot_Adam_1859.txt\n",
      "Gaskell_Lovers_1863.txt\n",
      "Corelli_Romance_1886.txt\n",
      "Conrad_Rover_1923.txt\n",
      "Gissing_Women_1893.txt\n",
      "Woolf_Years_1937.txt\n",
      "Trollope_Phineas_1869.txt\n",
      "Lytton_Kenelm_1873.txt\n",
      "Blackmore_Springhaven_1887.txt\n",
      "Forster_View_1908.txt\n",
      "Eliot_Felix_1866.txt\n",
      "Chesterton_Napoleon_1904.txt\n",
      "Bennet_Imperial_1930.txt\n",
      "Burnett_Princess_1905.txt\n",
      "Ward_Milly_1881.txt\n",
      "Ford_Girl_1907.txt\n",
      "Meredith_Feverel_1859.txt\n",
      "Lee_Albany_1884.txt\n",
      "Ford_Soldier_1915.txt\n",
      "Ward_Ashe_1905.txt\n",
      "Morris_Water_1897.txt\n",
      "Galsworthy_Saints_1919.txt\n",
      "Gissing_Unclassed_1884.txt\n",
      "Anon_Clara_1864.txt\n",
      "Hardy_Jude_1895.txt\n",
      "Dickens_Expectations_1861.txt\n",
      "Chesterton_Thursday_1908.txt\n",
      "Burnett_Lord_1886.txt\n",
      "Braddon_Phantom_1883.txt\n",
      "Gaskell_Ruth_1855.txt\n",
      "Kipling_Captains_1896.txt\n"
     ]
    }
   ],
   "source": [
    "filenames = [] #Creating an empty list called 'filenames'.\n",
    "\n",
    "for filename in Path(filepath).glob(\"*.txt\"): #Telling my script to load all .txt files in my specified filepath one-by-one.\n",
    "    print(os.path.basename(os.path.normpath(filename))) #Each time a file is loaded I print the filename. I specify that I only want the last entry of the path, namely the name of the novel. This line could be left out but is included to ensure that my data is loaded correctly.\n",
    "    filenames.append(os.path.basename(os.path.normpath(filename))) #Append the filename to my list called 'filenames'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2: Calculate the total number of unique words for each novel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "196557\n",
      "50477\n",
      "70185\n",
      "214985\n",
      "68448\n",
      "111057\n",
      "118088\n",
      "90672\n",
      "110455\n",
      "121950\n",
      "72479\n",
      "172276\n",
      "80291\n",
      "151197\n",
      "187049\n",
      "76281\n",
      "217694\n",
      "85093\n",
      "105920\n",
      "311335\n",
      "210553\n",
      "177917\n",
      "357936\n",
      "167075\n",
      "148763\n",
      "68397\n",
      "107663\n",
      "338512\n",
      "156151\n",
      "169571\n",
      "82948\n",
      "102106\n",
      "59639\n",
      "79361\n",
      "273259\n",
      "121506\n",
      "270014\n",
      "189103\n",
      "120704\n",
      "154305\n",
      "81043\n",
      "72055\n",
      "359496\n",
      "132759\n",
      "113770\n",
      "89871\n",
      "49983\n",
      "40015\n",
      "63257\n",
      "356604\n",
      "52644\n",
      "48242\n",
      "183180\n",
      "100645\n",
      "456592\n",
      "124497\n",
      "24612\n",
      "218572\n",
      "167555\n",
      "172356\n",
      "174199\n",
      "159489\n",
      "72102\n",
      "122382\n",
      "75043\n",
      "167016\n",
      "247078\n",
      "138440\n",
      "21840\n",
      "216651\n",
      "191037\n",
      "100526\n",
      "88101\n",
      "139234\n",
      "130903\n",
      "266634\n",
      "193074\n",
      "202113\n",
      "67930\n",
      "182816\n",
      "54920\n",
      "255975\n",
      "66877\n",
      "47588\n",
      "35708\n",
      "168781\n",
      "62913\n",
      "76750\n",
      "141832\n",
      "147737\n",
      "95156\n",
      "124877\n",
      "197620\n",
      "147273\n",
      "186804\n",
      "58299\n",
      "58698\n",
      "180676\n",
      "161797\n",
      "53467\n"
     ]
    }
   ],
   "source": [
    "#Loading data:\n",
    "total_words = [] #Creating an empty list called 'total_words'.\n",
    "\n",
    "for filename in Path(filepath).glob(\"*.txt\"): #Telling my script to load all .txt files in my specified filepath one-by-one.\n",
    "    with open(filename, \"r\", encoding=\"utf-8\") as file: #Loading the contents of the .txt-file.\n",
    "        loaded_text = file.read() #Reading the file and appending it to an object called 'loaded text'.\n",
    "        split_text = loaded_text.split() #Splitting the loaded text into individual words and append the words to an object called 'split_text'.\n",
    "        print(len(split_text)) #Print the amount of words (this is included to ensure that the code is working).\n",
    "        total_words.append(len(split_text)) #Append the amount of words to the list 'total_words'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16419\n",
      "5668\n",
      "6900\n",
      "16214\n",
      "6191\n",
      "9436\n",
      "8551\n",
      "6920\n",
      "9606\n",
      "12855\n",
      "7508\n",
      "12784\n",
      "7247\n",
      "12759\n",
      "12183\n",
      "8070\n",
      "9849\n",
      "7258\n",
      "8263\n",
      "16363\n",
      "11998\n",
      "13984\n",
      "16105\n",
      "10799\n",
      "10662\n",
      "6724\n",
      "10259\n",
      "19475\n",
      "14772\n",
      "12364\n",
      "7275\n",
      "7865\n",
      "5904\n",
      "8217\n",
      "13998\n",
      "7893\n",
      "12157\n",
      "14599\n",
      "7047\n",
      "7360\n",
      "5022\n",
      "8843\n",
      "18772\n",
      "9967\n",
      "9078\n",
      "7892\n",
      "3988\n",
      "4702\n",
      "6525\n",
      "17938\n",
      "6396\n",
      "6054\n",
      "12053\n",
      "7333\n",
      "20456\n",
      "10456\n",
      "2906\n",
      "15690\n",
      "9197\n",
      "11623\n",
      "10193\n",
      "11168\n",
      "7158\n",
      "8141\n",
      "8587\n",
      "10754\n",
      "10619\n",
      "12349\n",
      "3677\n",
      "12662\n",
      "11859\n",
      "8901\n",
      "7491\n",
      "8976\n",
      "8672\n",
      "9752\n",
      "13746\n",
      "12973\n",
      "6992\n",
      "13128\n",
      "6666\n",
      "15308\n",
      "5030\n",
      "3541\n",
      "4883\n",
      "13835\n",
      "8107\n",
      "7008\n",
      "12829\n",
      "6688\n",
      "7984\n",
      "8702\n",
      "14354\n",
      "11355\n",
      "11454\n",
      "6449\n",
      "4690\n",
      "12939\n",
      "10053\n",
      "7269\n"
     ]
    }
   ],
   "source": [
    "#Loading data:\n",
    "unique_words = [] ##Creating an empty list called 'unique_words'.\n",
    "\n",
    "for filename in Path(filepath).glob(\"*.txt\"): #Telling my script to load all .txt files in my specified filepath one-by-one.\n",
    "    with open(filename, \"r\", encoding=\"utf-8\") as file: #Loading the contents of the .txt-file.\n",
    "        loaded_text = file.read() #Reading the file and appending it to an object called 'loaded text'.\n",
    "        loaded_text = loaded_text.lower() #Using the lower-method to make all letters in the text lowercase to ensure that identical words with different casing are regarded as the same word.\n",
    "        regex = re.compile('[^a-zA-Z\\s:]') #Use regular expression to define special characters which I want to delete. \\s is whitespace, a-zA-Z - matches all the letters, ^ - negates them all so it deletes everything else. This is done to ensure that, i.e., \"fish.\" and \"fish!\" are recognized as identical words.\n",
    "        loaded_text = regex.sub('', loaded_text) #Save this in an object called 'loaded_text'.\n",
    "        split_text = loaded_text.split() #Split loaded text into individual words and save this in an object called 'split_text'.\n",
    "        print(len(Counter(split_text).keys())) #Use the counter-function to count each unique word. Print the length of this (ensuring that everyhting is working.\n",
    "        unique_words.append(len(Counter(split_text).keys())) #Append the amount of uique words in the text to the list 'unique_words'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3: Save result as a single file consisting of three columns: filename, total_words, unique_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict = {'filename': filenames, 'total_words': total_words, 'unique_words': unique_words} #Creating column names and contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(dict) #Creating a pandas-dataframe using the above specifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('file1.csv') #Write the dataframe as a .csv-file called 'file1'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('file1.csv', index_col=0) #Loading the dataframe to ensure that it was writen correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        filename  total_words  unique_words\n",
      "0      Cbronte_Villette_1853.txt       196557         16419\n",
      "1        Forster_Angels_1905.txt        50477          5668\n",
      "2      Woolf_Lighthouse_1927.txt        70185          6900\n",
      "3     Meredith_Richmond_1871.txt       214985         16214\n",
      "4    Stevenson_Treasure_1883.txt        68448          6191\n",
      "..                           ...          ...           ...\n",
      "95  Chesterton_Thursday_1908.txt        58299          6449\n",
      "96         Burnett_Lord_1886.txt        58698          4690\n",
      "97      Braddon_Phantom_1883.txt       180676         12939\n",
      "98         Gaskell_Ruth_1855.txt       161797         10053\n",
      "99     Kipling_Captains_1896.txt        53467          7269\n",
      "\n",
      "[100 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df_test) #Success."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lang101",
   "language": "python",
   "name": "lang101"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
